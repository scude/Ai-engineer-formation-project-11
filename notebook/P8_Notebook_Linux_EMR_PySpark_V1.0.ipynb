{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie cloud EMR\n",
    "\n",
    "Ce notebook reprend à partir de la section 4.10 (exécution sur EMR)."
   ],
   "id": "2b878195b36eb24"
  },
  {
   "cell_type": "markdown",
   "id": "4e759c1e",
   "metadata": {},
   "source": [
    "### 4.10.1 Démarrage de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "id": "e5f0fbe1",
   "metadata": {},
   "source": [
    "# L'exécution de cette cellule démarre l'application Spark"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3aba202f",
   "metadata": {},
   "source": [
    "<u>Affichage des informations sur la session en cours et liens vers Spark UI</u> :"
   ]
  },
  {
   "cell_type": "code",
   "id": "fb788991",
   "metadata": {},
   "source": [
    "%%info"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27ac9832",
   "metadata": {},
   "source": [
    "### 4.10.2 Installation des packages\n",
    "\n",
    "Les packages nécessaires ont été installé via l'étape de **bootstrap** à l'instanciation du serveur.\n",
    "\n",
    "### 4.10.3 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "id": "ad562eab",
   "metadata": {},
   "source": [
    "# Pour lire les images qui auront été chargées au format binaire :\n",
    "import io\n",
    "# Pour la gestion des chemins de fichiers\n",
    "import os\n",
    "\n",
    "# Pour valider les résultats et les exporter en CSV\n",
    "import pandas as pd\n",
    "# Pour redimensionner les images\n",
    "from PIL import Image\n",
    "# Pour la manipulation d'arrays\n",
    "import numpy as np\n",
    "\n",
    "# Désactivation des messages de debugging de tensorflow.\n",
    "# Doit être exécuté avant les imports de tensorflow.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Pour l'extraction de features des images\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Création et manipulation de dataframes Spark\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Pour convertion des features en vecteurs Spark\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "# Pour réalisation de la PCA sur les features\n",
    "from pyspark.ml.feature import PCA"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "83663cbd",
   "metadata": {},
   "source": [
    "### 4.10.4 Définition des PATH pour charger les images et enregistrer les résultats\n",
    "\n",
    "Nous accédons directement à nos **données sur S3** comme si elles étaient **stockées localement**."
   ]
  },
  {
   "cell_type": "code",
   "id": "46be859d",
   "metadata": {},
   "source": [
    "# Root bucket (project-level)\n",
    "S3_BUCKET = \"s3://p11-fruits-710002907257-eu-west-3\"\n",
    "\n",
    "# Dataset root\n",
    "PATH = f\"{S3_BUCKET}/data/fruits-360\"\n",
    "\n",
    "# Input data\n",
    "PATH_Data = f\"{PATH}/Test\"        # ou /Training selon l'étape\n",
    "\n",
    "# Outputs\n",
    "PATH_Result = f\"{S3_BUCKET}/results\"\n",
    "\n",
    "print(\n",
    "    \"PATH:                 \" + PATH +\n",
    "    \"\\nPATH_Data:            \" + PATH_Data +\n",
    "    \"\\nPATH_Result:          \" + PATH_Result\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf883c20",
   "metadata": {},
   "source": [
    "### 4.10.5 Traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe93f5",
   "metadata": {},
   "source": [
    "#### 4.10.5.1 Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "id": "7e4b319a",
   "metadata": {},
   "source": [
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aperçu rapide des premières lignes pour vérifier que le chargement des images a bien fonctionné.\n"
   ],
   "id": "8b77b4d103ed38dc"
  },
  {
   "cell_type": "code",
   "id": "16bfeb4d",
   "metadata": {},
   "source": [
    "images.show(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b32ac34",
   "metadata": {},
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n",
    "    une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "id": "a52ab808",
   "metadata": {},
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f15b199",
   "metadata": {},
   "source": [
    "#### 4.10.5.2 Préparation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec7c7165",
   "metadata": {},
   "source": [
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction du nouveau modèle qui sort les features depuis MobileNetV2.\n"
   ],
   "id": "84c3ddba026dd8e5"
  },
  {
   "cell_type": "code",
   "id": "1b9bc650",
   "metadata": {},
   "source": [
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion (broadcast) des poids du modèle aux exécutants Spark pour éviter des envois répétés.\n"
   ],
   "id": "bda0cb0172ae0457"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "broadcast_weights = sc.broadcast(new_model.get_weights())",
   "id": "a0d497f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résumé du modèle pour valider l’architecture et les dimensions de sortie.\n"
   ],
   "id": "a9e29b56e67926d8"
  },
  {
   "cell_type": "code",
   "id": "1bc0bf14",
   "metadata": {},
   "source": [
    "new_model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction utilitaire qui reconstitue le modèle sur les workers à partir des poids diffusés.\n"
   ],
   "id": "36734f61c3bfb59b"
  },
  {
   "cell_type": "code",
   "id": "be8fe2b9",
   "metadata": {},
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights=None,\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(broadcast_weights.value)\n",
    "    return new_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c032f135",
   "metadata": {},
   "source": [
    "#### 4.10.5.3 Définition du processus de chargement des images <br/> et application de leur featurisation à travers l'utilisation de pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "id": "933100cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).convert('RGB').resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series, batch_size=32):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    batch = np.stack(content_series.map(preprocess))\n",
    "    preds = model(batch, training=False).numpy()\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten().astype(np.float32).tolist() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f23206e8",
   "metadata": {},
   "source": [
    "#### 4.10.5.4 Exécutions des actions d'extractions de features"
   ]
  },
  {
   "cell_type": "code",
   "id": "22d760c2",
   "metadata": {},
   "source": " spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction des features via la Pandas UDF et mise en forme du DataFrame résultant.\n"
   ],
   "id": "81fab09ca53b9216"
  },
  {
   "cell_type": "code",
   "id": "5e07fd68",
   "metadata": {},
   "source": [
    "features_df = images.repartition(20).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification du schéma du DataFrame de features.\n"
   ],
   "id": "401e756fe109f672"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "features_df.printSchema()",
   "id": "51cd05e64cf4a7b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "772f9e71",
   "metadata": {},
   "source": [
    "### 4.10.6 PCA (MLlib) après extraction de features\n",
    "\n",
    "Même logique que la section locale : nous passons d'un vecteur 1280D \n",
    "à un espace plus compact (ex. `PCA_K=50`). Cela prépare la suite du projet (classification, indexation).\n",
    "\n",
    "> **Paramétrage mémoire** : `PCA_SAMPLE_FRACTION` permet d'entraîner la PCA sur\n",
    "> un sous-échantillon pour limiter la consommation mémoire.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bfa96b65",
   "metadata": {},
   "source": [
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "features_df = features_df.withColumn('features', list_to_vector_udf('features'))\n",
    "\n",
    "n_componants = 28\n",
    "\n",
    "pca = PCA(k=n_componants, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(features_df)\n",
    "features_df = model.transform(features_df).select(\"path\", \"label\", \"features\", \"pcaFeatures\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage du pourcentage de variance expliqué par la PCA.\n"
   ],
   "id": "8e5109185b5c0f5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"{n_componants} composantes captent {sum(model.explainedVariance)*100:.2f} % de la variance.\")",
   "id": "66e0a013eff200c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification du chemin de sortie avant l’écriture.\n"
   ],
   "id": "e282049d3cee94ce"
  },
  {
   "cell_type": "code",
   "id": "06a930b3",
   "metadata": {},
   "source": [
    "print(PATH_Result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde des features (et composantes PCA) au format Parquet.\n"
   ],
   "id": "50ff2df59ab115c6"
  },
  {
   "cell_type": "code",
   "id": "7c53ddd5",
   "metadata": {},
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1fe01b72",
   "metadata": {},
   "source": [
    "### 4.10.6 Chargement des données enregistrées et validation du résultat"
   ]
  },
  {
   "cell_type": "code",
   "id": "db18a784",
   "metadata": {},
   "source": [
    "spark.read.parquet(PATH_Result).show(5, truncate=False)\n",
    "\n",
    "# Optionnel: conversion en pandas sur un petit échantillon uniquement\n",
    "sample_pdf = spark.read.parquet(PATH_Result).limit(10).toPandas()\n",
    "sample_pdf.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement local (Pandas) du Parquet pour contrôles rapides.\n"
   ],
   "id": "57495853606af092"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet(PATH_Result, engine='pyarrow')\n",
    "# Séparer les éléments de 'path' et ne garder que le dernier (nom du fichier)\n",
    "df['path'] = df['path'].apply(lambda x: x.split('/')[-1])\n",
    "# Renommer la colonne 'path' en 'filename'\n",
    "df = df.rename(columns={'path': 'filename'})"
   ],
   "id": "3352771ac0b3121d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection d’un exemple de vecteur de features.\n"
   ],
   "id": "ad96f3d1d3562668"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['features'][0]",
   "id": "14d8166012a2c2d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction des valeurs numériques depuis les structures Spark.\n"
   ],
   "id": "635ce3ed650e13b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['features'] = df['features'].apply(lambda x: x['values'] if x is not None else None)\n",
    "df['pcaFeatures'] = df['pcaFeatures'].apply(lambda x: x['values'] if x is not None else None)"
   ],
   "id": "4d3e0d751d2dc1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrôle de la dimension du vecteur de features.\n"
   ],
   "id": "8029d5e987f42dd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.loc[0,'features'].shape",
   "id": "1b62970b3f98458b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrôle de la dimension après PCA.\n"
   ],
   "id": "8412fa4263e5858a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.loc[0,'pcaFeatures'].shape",
   "id": "284d7a25fb5f9d5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aperçu des premières lignes du DataFrame Pandas.\n"
   ],
   "id": "69d3e601933eeff8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "df.head()"
   ],
   "id": "86e1fe00303f6dfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustement de l’affichage NumPy pour éviter les retours à la ligne dans les vecteurs.\n"
   ],
   "id": "ca7d79cdb28b7353"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# np.printoptions pour éviter l'insertion de \"\\n\" dans 'pcaFeatures' dans notre fichier csv\n",
    "with np.printoptions(linewidth=10000):\n",
    "    df[['filename', 'label', 'pcaFeatures']].to_csv(PATH_Result+'/'+'pcaFeatures.csv', index=False, sep='\\t')"
   ],
   "id": "f00c03f1ebcfb3b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "72974aab",
   "metadata": {},
   "source": [
    "<u>On peut également constater la présence des fichiers <br />\n",
    "    au format \"**parquet**\" sur le **serveur S3**</u> :\n",
    "\n",
    "![Affichage des résultats sur S3](data/img/S3_Results.png)\n",
    "\n",
    "## 4.11 Suivi de l'avancement des tâches avec le Serveur d'Historique Spark\n",
    "\n",
    "Il est possible de voir l'avancement des tâches en cours <br />\n",
    "avec le **serveur d'historique Spark**.\n",
    "\n",
    "![Accès au serveur d'historique spark](data/img/EMR_serveur_historique_spark_acces.png)\n",
    "\n",
    "**Il est également possible de revenir et d'étudier les tâches <br />\n",
    "qui ont été réalisé, afin de debugger, optimiser les futurs <br />\n",
    "tâches à réaliser.**\n",
    "\n",
    "<u>Lorsque la commande \"**features_df.write.mode(\"overwrite\").parquet(PATH_Result)**\" <br />\n",
    "était en cours, nous pouvions observer son état d'avancement</u> :\n",
    "\n",
    "![Progression execution script](data/img/EMR_jupyterhub_avancement.png)\n",
    "\n",
    "<u>Le **serveur d'historique Spark** nous permet une vision beaucoup plus précise <br />\n",
    "de l'exécution des différentes tâche sur les différentes machines du cluster</u> :\n",
    "\n",
    "![Suivi des tâches spark](data/img/EMR_SHSpark_01.png)\n",
    "\n",
    "On peut également constater que notre cluster de calcul a mis <br />\n",
    "un tout petit peu **moins de 8 minutes** pour traiter les **22 688 images**.\n",
    "\n",
    "![Temps de traitement](data/img/EMR_SHSpark_02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d65bf",
   "metadata": {},
   "source": [
    "## 4.12 Résiliation de l'instance EMR\n",
    "\n",
    "Notre travail est maintenant terminé. <br />\n",
    "Le cluster de machines EMR est **facturé à la demande**, <br />\n",
    "et nous continuons d'être facturé même lorsque <br />\n",
    "les machines sont au repos.<br />\n",
    "Pour **optimiser la facturation**, il nous faut <br />\n",
    "maintenant **résilier le cluster**.\n",
    "\n",
    "<u>Je réalise cette commande depuis l'interface AWS</u> :\n",
    "\n",
    "1. Commencez par **désactiver le tunnel ssh dans FoxyProxy** pour éviter des problèmes de **timeout**.\n",
    "![Désactivation de FoxyProxy](data/img/EMR_foxyproxy_desactivation.png)\n",
    "2. Cliquez sur \"**Résilier**\"\n",
    "![Cliquez sur Résilier](data/img/EMR_resiliation_01.png)\n",
    "3. Confirmez la résiliation\n",
    "![Confirmez la résiliation](data/img/EMR_resiliation_02.png)\n",
    "4. La résiliation prend environ **1 minute**\n",
    "![Résiliation en cours](data/img/EMR_resiliation_03.png)\n",
    "5. La résiliation est effectuée\n",
    "![Résiliation terminée](data/img/EMR_resiliation_04.png)\n",
    "\n",
    "**Checklist coût / cluster éphémère** :\n",
    "- Créer le cluster uniquement pour les tests/démos.\n",
    "- Vérifier que les jobs sont terminés (Spark UI).\n",
    "- Sauvegarder les logs nécessaires dans S3.\n",
    "- Résilier immédiatement le cluster après exécution.\n",
    "\n",
    "## 4.13 Cloner le serveur EMR (si besoin)\n",
    "\n",
    "Si nous devons de nouveau exécuter notre notebook dans les mêmes conditions, <br />\n",
    "il nous suffit de **cloner notre cluster** et ainsi en obtenir une copie fonctionnelle <br />\n",
    "sous 15/20 minutes, le temps de son instanciation.\n",
    "\n",
    "<u>Pour cela deux solutions</u> :\n",
    "1. <u>Depuis l'interface AWS</u> :\n",
    " 1. Cliquez sur \"**Cloner**\"\n",
    "   ![Cloner un cluster](data/img/EMR_cloner_01.png)\n",
    " 2. Dans notre cas nous ne souhaitons pas inclure d'étapes\n",
    "   ![Ne pas inclure d'étapes](data/img/EMR_cloner_02.png)\n",
    " 3. La configuration du cluster est recréée à l’identique. <br />\n",
    "    On peut revenir sur les différentes étapes si on souhaite apporter des modifications<br />\n",
    "    Quand tout est prêt, cliquez sur \"**Créer un cluster**\"\n",
    "  ![Vérification/Modification/Créer un cluster](data/img/EMR_cloner_03.png)\n",
    "2. <u>En ligne de commande</u> (avec AWS CLI d'installé et de configuré et en s'assurant <br />\n",
    "   de s'attribuer les droits nécessaires sur le compte AMI utilisé)\n",
    " 1. Cliquez sur \"**Exporter AWS CLI**\"\n",
    " ![Exporter AWS CLI](data/img/EMR_cloner_cli_01.png)\n",
    " 2. Copier/Coller la commande **depuis un terminal**\n",
    " ![Copier Coller Commande](data/img/EMR_cloner_cli_02.png)\n",
    "\n",
    "## 4.14 Arborescence du serveur S3 à la fin du projet\n",
    "\n",
    "<u>Pour information, voici **l'arborescence complète de mon bucket S3 p11-data** à la fin du projet</u> : <br />\n",
    "*Par soucis de lisibilité, je ne liste pas les 131 sous dossiers du répertoire \"Test\"*\n",
    "\n",
    "1. Results/_SUCCESS\n",
    "1. Results/part-00000-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00001-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00002-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00003-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00004-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00005-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00006-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00007-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00008-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00009-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00010-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00011-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00012-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00013-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00014-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00015-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00016-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00017-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00018-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00019-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00020-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00021-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00022-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00023-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Test/\n",
    "1. bootstrap-emr.sh\n",
    "1. jupyter-s3-conf.json\n",
    "1. jupyter/jovyan/.s3keep\n",
    "1. jupyter/jovyan/P11_01_Notebook.ipynb\n",
    "1. jupyter/jovyan/_metadata\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/file-perm.sqlite\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/html/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/latex/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbsignatures.db\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/notebook_secret\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled1-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/test3-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled1.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/test3.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba46f9",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "Nous avons réalisé ce projet **en deux temps** en tenant <br />\n",
    "compte des contraintes qui nous ont été imposées.\n",
    "\n",
    "Nous avons **dans un premier temps développé notre solution en local** <br />\n",
    "sur une machine virtuelle dans un environnement Linux Ubuntu.\n",
    "\n",
    "La <u>première phase</u> a consisté à **installer l'environnement de travail Spark**. <br />\n",
    "**Spark** a un paramètre qui nous permet de travaillé en local et nous permet <br />\n",
    "ainsi de **simuler du calcul partagé** en considérant <br />\n",
    "**chaque cœur d'un processeur comme un worker indépendant**.<br />\n",
    "Nous avons travaillé sur un plus **petit jeu de donnée**, l'idée était <br />\n",
    "simplement de **valider le bon fonctionnement de la solution**.\n",
    "\n",
    "Nous avons fait le choix de réaliser du **transfert learning** <br />\n",
    "à partir du model **MobileNetV2**.<br />\n",
    "Ce modèle a été retenu pour sa **légèreté** et sa **rapidité d'exécution** <br />\n",
    "ainsi que pour la **faible dimension de son vecteur en sortie**.\n",
    "\n",
    "Les résultats ont été enregistrés sur disque en plusieurs <br />\n",
    "partitions au format \"**parquet**\".\n",
    "\n",
    "<u>**La solution a parfaitement fonctionné en mode local**</u>.\n",
    "\n",
    "La <u>deuxième phase</u> a consisté à créer un **réel cluster de calculs**. <br />\n",
    "L'objectif était de pouvoir **anticiper une future augmentation de la charge de travail**.\n",
    "\n",
    "Le meilleur choix retenu a été l'utilisation du prestataire de services **Amazon Web Services** <br />\n",
    "qui nous permet de **louer à la demande de la puissance de calculs**, <br />\n",
    "pour un **coût tout à fait acceptable**.<br />\n",
    "Ce service se nomme **EC2** et se classe parmi les offres **Infrastructure As A Service** (IAAS).\n",
    "\n",
    "Nous sommes allez plus loin en utilisant un service de plus <br />\n",
    "haut niveau (**Plateforme As A Service** PAAS)<br />\n",
    "en utilisant le service **EMR** qui nous permet d'un seul coup <br />\n",
    "d'**instancier plusieurs serveur (un cluster)** sur lesquels <br />\n",
    "nous avons pu demander l'installation et la configuration de plusieurs<br />\n",
    "programmes et librairies nécessaires à notre projet comme **Spark**, <br />\n",
    "**Hadoop**, **JupyterHub** ainsi que la librairie **TensorFlow**.\n",
    "\n",
    "En plus d'être plus **rapide et efficace à mettre en place**, nous avons <br />\n",
    "la **certitude du bon fonctionnement de la solution**, celle-ci ayant été <br />\n",
    "préalablement validé par les ingénieurs d'Amazon.\n",
    "\n",
    "Nous avons également pu installer, sans difficulté, **les packages <br />\n",
    "nécessaires sur l'ensembles des machines du cluster**.\n",
    "\n",
    "Enfin, avec très peu de modification, et plus simplement encore, <br />\n",
    "nous avons pu **exécuter notre notebook comme nous l'avions fait localement**.<br />\n",
    "Nous avons cette fois-ci exécuté le traitement sur **l'ensemble des images de notre dossier \"Test\"**.\n",
    "\n",
    "Nous avons opté pour le service **Amazon S3** pour **stocker les données de notre projet**. <br />\n",
    "S3 offre, pour un faible coût, toutes les conditions dont nous avons besoin pour stocker <br />\n",
    "et exploiter de manière efficace nos données.<br />\n",
    "L'espace alloué est potentiellement **illimité**, mais les coûts seront fonction de l'espace utilisé.\n",
    "\n",
    "Il nous sera **facile de faire face à une monté de la charge de travail** en **redimensionnant** <br />\n",
    "simplement notre cluster de machines (horizontalement et/ou verticalement au besoin), <br />\n",
    "les coûts augmenteront en conséquence mais resteront nettement inférieurs aux coûts engendrés <br />\n",
    "par l'achat de matériels ou par la location de serveurs dédiés."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "432.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
